{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVhgzOlFRpw_",
        "outputId": "8a42d56b-4681-4f3e-d9f5-9cb55b2a4bc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /packages/b2/84/e74d5b741e253343d30f4f01364f4367e21a402514503122d1065d5e7b75/autogluon.common-1.4.0-py3-none-any.whl.metadata\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.9/454.9 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.3/487.3 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.7/189.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.4/287.4 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.5/88.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m828.5/828.5 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.1/68.1 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m353.3/353.3 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.0/821.0 MB\u001b[0m \u001b[31m537.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.5/963.5 kB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m120.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m832.4/832.4 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m106.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.5/201.5 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!python -m pip install -q autogluon;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg4TueF2P9Lr",
        "outputId": "1e68fa94-5c5d-41e3-cc11-fd07b12bc1b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Building time-series features...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.4.0\n",
            "Python Version:     3.12.11\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Sun Mar 30 16:01:29 UTC 2025\n",
            "CPU Count:          2\n",
            "Memory Avail:       6.87 GB / 12.67 GB (54.2%)\n",
            "Disk Space Avail:   62.32 GB / 107.72 GB (57.9%)\n",
            "===================================================\n",
            "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
            "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
            "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
            "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
            "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
            "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
            "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New feature count: 183\n",
            "\n",
            "--- Processing Target: Y1 ---\n",
            "  Training temporary model for Y1 to find important features...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Beginning AutoGluon training ... Time limit = 60s\n",
            "AutoGluon will save models to \"/content/temp_feature_model_Y1_1758546400\"\n",
            "Train Data Rows:    80000\n",
            "Train Data Columns: 183\n",
            "Label Column:       Y1\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (29.859458454056327, -28.9181795461607, -0.00281, 0.97066)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    7124.23 MB\n",
            "\tTrain Data (Original)  Memory Usage: 111.69 MB (1.6% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 182 | ['A', 'B', 'C', 'D', 'E', ...]\n",
            "\t\t('int', [])   :   1 | ['time']\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 182 | ['A', 'B', 'C', 'D', 'E', ...]\n",
            "\t\t('int', [])   :   1 | ['time']\n",
            "\t1.9s = Fit runtime\n",
            "\t183 features in original data used to generate 183 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 111.69 MB (1.6% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 2.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.03125, Train Rows: 77500, Val Rows: 2500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'GBM': [{}],\n",
            "}\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM ... Training model for up to 57.82s of the 57.82s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.6/6.7 GB\n",
            "\tRan out of time, early stopping on iteration 721. Best iteration is:\n",
            "\t[629]\tvalid_set's l1: 0.290544\n",
            "\t-0.2905\t = Validation score   (-mean_absolute_error)\n",
            "\t57.93s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 57.82s of the -0.30s of remaining time.\n",
            "\tEnsemble Weights: {'LightGBM': 1.0}\n",
            "\t-0.2905\t = Validation score   (-mean_absolute_error)\n",
            "\t0.0s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 60.66s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 26005.6 rows/s (2500 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/temp_feature_model_Y1_1758546400\")\n",
            "Computing feature importance via permutation shuffling for 183 features using 5000 rows with 5 shuffle sets...\n",
            "\t190.19s\t= Expected runtime (38.04s per shuffle set)\n",
            "\t210.36s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n",
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"AutogluonModels_Y1\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.4.0\n",
            "Python Version:     3.12.11\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Sun Mar 30 16:01:29 UTC 2025\n",
            "CPU Count:          2\n",
            "Memory Avail:       6.79 GB / 12.67 GB (53.6%)\n",
            "Disk Space Avail:   62.32 GB / 107.72 GB (57.9%)\n",
            "===================================================\n",
            "Presets specified: ['best_quality']\n",
            "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
            "\tRunning DyStack for up to 90s of the 360s of remaining time (25%).\n",
            "\t\tContext path: \"/content/AutogluonModels_Y1/ds_sub_fit/sub_fit_ho\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Cleaned up temporary directory: temp_feature_model_Y1_1758546400\n",
            "Selected 14 Features for Y1: ['G', 'H', 'J', 'M', 'E', 'C', 'time', 'N', 'F_diff1', 'G_diff1', 'A_diff1', 'K_diff1', 'M_lag1', 'K']\n",
            "  Training main model for Y1. Results will be in 'AutogluonModels_Y1'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Leaderboard on holdout data (DyStack):\n",
            "                 model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  WeightedEnsemble_L3       0.788088   0.759760          r2        3.886721       1.678365  89.384497                 0.002659                0.002788           0.185765            3       True          4\n",
            "1      LightGBM_BAG_L1       0.788073   0.759524          r2        3.595663       1.141915  47.276585                 3.595663                1.141915          47.276585            1       True          1\n",
            "2  WeightedEnsemble_L2       0.788073   0.759524          r2        3.598068       1.144186  47.282189                 0.002405                0.002271           0.005604            2       True          2\n",
            "3      LightGBM_BAG_L2       0.785637   0.755682          r2        3.884063       1.675577  89.198732                 0.288400                0.533663          41.922147            2       True          3\n",
            "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
            "\t122s\t = DyStack   runtime |\t238s\t = Remaining runtime\n",
            "Starting main fit with num_stack_levels=1.\n",
            "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
            "Beginning AutoGluon training ... Time limit = 238s\n",
            "AutoGluon will save models to \"/content/AutogluonModels_Y1\"\n",
            "Train Data Rows:    80000\n",
            "Train Data Columns: 14\n",
            "Label Column:       Y1\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    6529.04 MB\n",
            "\tTrain Data (Original)  Memory Usage: 8.55 MB (0.1% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 13 | ['G', 'H', 'J', 'M', 'E', ...]\n",
            "\t\t('int', [])   :  1 | ['time']\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 13 | ['G', 'H', 'J', 'M', 'E', ...]\n",
            "\t\t('int', [])   :  1 | ['time']\n",
            "\t0.4s = Fit runtime\n",
            "\t14 features in original data used to generate 14 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 8.55 MB (0.1% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.4s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'GBM': [{}],\n",
            "\t'CAT': [{}],\n",
            "\t'XGB': [{}],\n",
            "\t'RF': [{}],\n",
            "\t'XT': [{}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 5 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 158.64s of the 238.01s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.91%)\n",
            "\t0.7631\t = Validation score   (r2)\n",
            "\t51.58s\t = Training   runtime\n",
            "\t1.21s\t = Validation runtime\n",
            "Fitting model: RandomForest_BAG_L1 ... Training model for up to 102.16s of the 181.54s of remaining time.\n",
            "\tWarning: Reducing model 'n_estimators' from 300 -> 47 due to low time. Expected time usage reduced from 644.1s -> 102.1s...\n",
            "\t0.7468\t = Validation score   (r2)\n",
            "\t105.92s\t = Training   runtime\n",
            "\t0.87s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 238.02s of the 74.35s of remaining time.\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.727, 'RandomForest_BAG_L1': 0.273}\n",
            "\t0.7659\t = Validation score   (r2)\n",
            "\t0.12s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 5 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM_BAG_L2 ... Training model for up to 74.20s of the 74.19s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.92%)\n",
            "\t0.7607\t = Validation score   (r2)\n",
            "\t44.5s\t = Training   runtime\n",
            "\t0.61s\t = Validation runtime\n",
            "Fitting model: RandomForest_BAG_L2 ... Training model for up to 23.52s of the 23.51s of remaining time.\n",
            "\tWarning: Model is expected to require 1251.9s to train, which exceeds the maximum time limit of 23.4s, skipping model...\n",
            "\tTime limit exceeded... Skipping RandomForest_BAG_L2.\n",
            "Fitting model: CatBoost_BAG_L2 ... Training model for up to 6.37s of the 6.37s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=6.53%)\n",
            "\tTime limit exceeded... Skipping CatBoost_BAG_L2.\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 238.02s of the -3.65s of remaining time.\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.667, 'RandomForest_BAG_L1': 0.267, 'LightGBM_BAG_L2': 0.067}\n",
            "\t0.7659\t = Validation score   (r2)\n",
            "\t0.2s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 242.35s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 5199.1 rows/s (10000 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/AutogluonModels_Y1\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Predicting Y1 on test data...\n",
            "  Getting validation R² score for Y1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-22 13:17:22,880\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.4.0\n",
            "Python Version:     3.12.11\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Sun Mar 30 16:01:29 UTC 2025\n",
            "CPU Count:          2\n",
            "Memory Avail:       6.71 GB / 12.67 GB (53.0%)\n",
            "Disk Space Avail:   62.31 GB / 107.72 GB (57.9%)\n",
            "===================================================\n",
            "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
            "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
            "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
            "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
            "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
            "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
            "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Leaderboard for Y1:\n",
            "                 model  score_test  score_val eval_metric  pred_time_test  \\\n",
            "0  RandomForest_BAG_L1    0.961027   0.746777          r2        1.435604   \n",
            "1  WeightedEnsemble_L3    0.866156   0.765925          r2       11.269676   \n",
            "2  WeightedEnsemble_L2    0.865871   0.765901          r2        7.562623   \n",
            "3      LightGBM_BAG_L2    0.832428   0.760683          r2       11.264822   \n",
            "4      LightGBM_BAG_L1    0.811237   0.763108          r2        6.121696   \n",
            "\n",
            "   pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
            "0       0.872779  105.916589                 1.435604                0.872779   \n",
            "1       2.690001  202.190048                 0.004854                0.003322   \n",
            "2       2.082565  157.613549                 0.005322                0.003343   \n",
            "3       2.686679  201.993702                 3.707521                0.607456   \n",
            "4       1.206444   51.577542                 6.121696                1.206444   \n",
            "\n",
            "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
            "0         105.916589            1       True          2  \n",
            "1           0.196346            3       True          5  \n",
            "2           0.119418            2       True          3  \n",
            "3          44.499572            2       True          4  \n",
            "4          51.577542            1       True          1  \n",
            "\n",
            "--- Processing Target: Y2 ---\n",
            "  Training temporary model for Y2 to find important features...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Beginning AutoGluon training ... Time limit = 60s\n",
            "AutoGluon will save models to \"/content/temp_feature_model_Y2_1758547051\"\n",
            "Train Data Rows:    80000\n",
            "Train Data Columns: 183\n",
            "Label Column:       Y2\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (34.63603924182771, -0.8507991164008696, -0.06117, 0.9237)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    6968.89 MB\n",
            "\tTrain Data (Original)  Memory Usage: 111.69 MB (1.6% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 182 | ['A', 'B', 'C', 'D', 'E', ...]\n",
            "\t\t('int', [])   :   1 | ['time']\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 182 | ['A', 'B', 'C', 'D', 'E', ...]\n",
            "\t\t('int', [])   :   1 | ['time']\n",
            "\t2.5s = Fit runtime\n",
            "\t183 features in original data used to generate 183 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 111.69 MB (1.6% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 2.98s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.03125, Train Rows: 77500, Val Rows: 2500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'GBM': [{}],\n",
            "}\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM ... Training model for up to 57.02s of the 57.02s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.6/6.6 GB\n",
            "\tRan out of time, early stopping on iteration 732. Best iteration is:\n",
            "\t[706]\tvalid_set's l1: 0.215406\n",
            "\t-0.2154\t = Validation score   (-mean_absolute_error)\n",
            "\t57.18s\t = Training   runtime\n",
            "\t0.14s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 57.02s of the -0.42s of remaining time.\n",
            "\tEnsemble Weights: {'LightGBM': 1.0}\n",
            "\t-0.2154\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 60.77s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 17653.7 rows/s (2500 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/temp_feature_model_Y2_1758547051\")\n",
            "Computing feature importance via permutation shuffling for 183 features using 5000 rows with 5 shuffle sets...\n",
            "\t274.79s\t= Expected runtime (54.96s per shuffle set)\n",
            "\t294.2s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.4.0\n",
            "Python Version:     3.12.11\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Sun Mar 30 16:01:29 UTC 2025\n",
            "CPU Count:          2\n",
            "Memory Avail:       6.71 GB / 12.67 GB (52.9%)\n",
            "Disk Space Avail:   62.31 GB / 107.72 GB (57.9%)\n",
            "===================================================\n",
            "Presets specified: ['best_quality']\n",
            "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
            "\tRunning DyStack for up to 90s of the 360s of remaining time (25%).\n",
            "\t\tContext path: \"/content/AutogluonModels_Y2/ds_sub_fit/sub_fit_ho\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Cleaned up temporary directory: temp_feature_model_Y2_1758547051\n",
            "Selected 40 Features for Y2: ['A', 'time', 'K', 'D', 'B', 'F', 'L', 'I', 'G', 'A_diff1', 'N_rolling_std_10', 'F_diff1', 'I_rolling_mean_10', 'A_lag1', 'A_rolling_mean_3', 'J_rolling_std_10', 'F_lag10', 'D_rolling_mean_10', 'E_rolling_mean_10', 'M_rolling_std_10', 'K_rolling_mean_10', 'G_rolling_mean_3', 'L_rolling_std_10', 'B_rolling_std_10', 'C', 'G_rolling_mean_5', 'B_lag10', 'H', 'L_rolling_mean_10', 'H_rolling_mean_10', 'K_diff1', 'D_diff1', 'M_rolling_mean_10', 'G_rolling_std_5', 'B_rolling_mean_10', 'L_rolling_mean_3', 'I_diff1', 'A_lag5', 'G_rolling_std_10', 'J_rolling_mean_10']\n",
            "  Training main model for Y2. Results will be in 'AutogluonModels_Y2'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Leaderboard on holdout data (DyStack):\n",
            "                 model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0      LightGBM_BAG_L1       0.797997   0.793257          r2        4.240934       4.115792  82.320615                 4.240934                4.115792          82.320615            1       True          1\n",
            "1  WeightedEnsemble_L3       0.797997   0.793257          r2        4.243035       4.122634  82.336178                 0.002101                0.006842           0.015563            3       True          3\n",
            "2  WeightedEnsemble_L2       0.797997   0.793257          r2        4.243443       4.119048  82.327836                 0.002509                0.003256           0.007221            2       True          2\n",
            "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
            "\t99s\t = DyStack   runtime |\t261s\t = Remaining runtime\n",
            "Starting main fit with num_stack_levels=1.\n",
            "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
            "Beginning AutoGluon training ... Time limit = 261s\n",
            "AutoGluon will save models to \"/content/AutogluonModels_Y2\"\n",
            "Train Data Rows:    80000\n",
            "Train Data Columns: 40\n",
            "Label Column:       Y2\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    6480.65 MB\n",
            "\tTrain Data (Original)  Memory Usage: 24.41 MB (0.4% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 39 | ['A', 'K', 'D', 'B', 'F', ...]\n",
            "\t\t('int', [])   :  1 | ['time']\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 39 | ['A', 'K', 'D', 'B', 'F', ...]\n",
            "\t\t('int', [])   :  1 | ['time']\n",
            "\t0.8s = Fit runtime\n",
            "\t40 features in original data used to generate 40 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 24.41 MB (0.4% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.9s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'GBM': [{}],\n",
            "\t'CAT': [{}],\n",
            "\t'XGB': [{}],\n",
            "\t'RF': [{}],\n",
            "\t'XT': [{}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 5 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 173.21s of the 259.87s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=2.52%)\n",
            "\t0.8059\t = Validation score   (r2)\n",
            "\t178.13s\t = Training   runtime\n",
            "\t15.85s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 259.88s of the 73.16s of remaining time.\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
            "\t0.8059\t = Validation score   (r2)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 5 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM_BAG_L2 ... Training model for up to 73.13s of the 73.11s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=2.59%)\n",
            "\t0.7936\t = Validation score   (r2)\n",
            "\t66.65s\t = Training   runtime\n",
            "\t0.91s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 259.88s of the -0.51s of remaining time.\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
            "\t0.8059\t = Validation score   (r2)\n",
            "\t0.08s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 261.43s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 630.8 rows/s (10000 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/AutogluonModels_Y2\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Predicting Y2 on test data...\n",
            "  Getting validation R² score for Y2...\n",
            "  Leaderboard for Y2:\n",
            "                 model  score_test  score_val eval_metric  pred_time_test  \\\n",
            "0      LightGBM_BAG_L1    0.928964   0.805913          r2       68.857419   \n",
            "1  WeightedEnsemble_L2    0.928964   0.805913          r2       68.860964   \n",
            "2  WeightedEnsemble_L3    0.928964   0.805913          r2       68.861440   \n",
            "3      LightGBM_BAG_L2    0.914892   0.793598          r2       73.174574   \n",
            "\n",
            "   pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
            "0      15.852228  178.131537                68.857419               15.852228   \n",
            "1      15.855789  178.139014                 0.003545                0.003561   \n",
            "2      15.854463  178.215857                 0.004022                0.002235   \n",
            "3      16.761705  244.784099                 4.317156                0.909477   \n",
            "\n",
            "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
            "0         178.131537            1       True          1  \n",
            "1           0.007477            2       True          2  \n",
            "2           0.084320            3       True          4  \n",
            "3          66.652562            2       True          3  \n",
            "\n",
            "--- Calculating Final Validation R² Scores ---\n",
            "Validation R² Score for Y1 (from Leaderboard): 0.7468\n",
            "Validation R² Score for Y2 (from Leaderboard): 0.8059\n",
            "Final Average Validation R² Score: 0.7763\n",
            "\n",
            "--- Creating Final Submission File ---\n",
            "Submission file created: submission_autogluon.csv\n",
            "       id        Y1        Y2\n",
            "80000 NaN  0.519573 -0.170549\n",
            "80001 NaN -0.220000 -0.286028\n",
            "80002 NaN -0.288580 -0.061530\n",
            "80003 NaN -0.460233  0.308164\n",
            "80004 NaN -0.961541  0.103084\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import shutil\n",
        "import warnings\n",
        "from autogluon.tabular import TabularPredictor\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "TIME_LIMIT_PER_MODEL = 1800 \n",
        "IMPORTANCE_THRESHOLD_RATIO = 0.01 \n",
        "TARGET_LABELS = ['Y1', 'Y2']\n",
        "\n",
        "print(\"Loading data...\")\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test (1).csv')\n",
        "\n",
        "test_ids = test_df['id']\n",
        "\n",
        "test_df = test_df.drop('id', axis=1)\n",
        "\n",
        "train_df = train_df.sort_values(by='time').reset_index(drop=True)\n",
        "test_df = test_df.sort_values(by='time').reset_index(drop=True)\n",
        "\n",
        "def build_features(df):\n",
        "    print(\"Building time-series features...\")\n",
        "    df_featured = df.copy()\n",
        "    core_features = [col for col in df.columns if 'A' <= col <= 'N']\n",
        "\n",
        "    for feature in core_features:\n",
        "        for i in [1, 2, 3, 5, 10]:\n",
        "            df_featured[f'{feature}_lag{i}'] = df_featured[feature].shift(i)\n",
        "        df_featured[f'{feature}_diff1'] = df_featured[feature].diff(1)\n",
        "        # Rolling Windows\n",
        "        for window in [3, 5, 10]:\n",
        "            df_featured[f'{feature}_rolling_mean_{window}'] = df_featured[feature].shift(1).rolling(window=window, min_periods=1).mean()\n",
        "            df_featured[f'{feature}_rolling_std_{window}'] = df_featured[feature].shift(1).rolling(window=window, min_periods=1).std()\n",
        "\n",
        "    df_featured = df_featured.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "    print(f\"New feature count: {len(df_featured.columns)}\")\n",
        "    return df_featured\n",
        "\n",
        "combined_df = pd.concat([train_df.drop(columns=TARGET_LABELS), test_df], ignore_index=True)\n",
        "combined_df_featured = build_features(combined_df)\n",
        "\n",
        "train_df_featured = combined_df_featured.iloc[:len(train_df)].copy()\n",
        "test_df_featured = combined_df_featured.iloc[len(train_df):].copy()\n",
        "\n",
        "train_df_featured[TARGET_LABELS] = train_df[TARGET_LABELS]\n",
        "\n",
        "\n",
        "def get_top_features(df, target, importance_threshold_ratio=0.01):\n",
        "    temp_model_path = f\"temp_feature_model_{target}_{int(time.time())}\"\n",
        "    print(f\"  Training temporary model for {target} to find important features...\")\n",
        "    other_targets = [col for col in TARGET_LABELS if col != target]\n",
        "    feature_selection_data = df.drop(columns=other_targets)\n",
        "\n",
        "    try:\n",
        "        feature_predictor = TabularPredictor(label=target, path=temp_model_path, eval_metric='mean_absolute_error')\\\n",
        "            .fit(train_data=feature_selection_data, hyperparameters={'GBM': {}}, time_limit=60)\n",
        "        importances = feature_predictor.feature_importance(data=feature_selection_data)\n",
        "        if not importances.empty:\n",
        "            max_importance = importances['importance'].max()\n",
        "            threshold = importance_threshold_ratio * max_importance\n",
        "            selected_features = importances[importances['importance'] >= threshold].index.tolist()\n",
        "        else: selected_features = []\n",
        "    finally:\n",
        "        if 'feature_predictor' in locals() and feature_predictor.path:\n",
        "            shutil.rmtree(feature_predictor.path)\n",
        "            print(f\"  Cleaned up temporary directory: {temp_model_path}\")\n",
        "\n",
        "    return selected_features\n",
        "\n",
        "all_predictions = {}\n",
        "validation_scores = {} \n",
        "\n",
        "for target_label in TARGET_LABELS:\n",
        "    print(f\"\\n--- Processing Target: {target_label} ---\")\n",
        "\n",
        "    important_features = get_top_features(train_df_featured, target_label, IMPORTANCE_THRESHOLD_RATIO)\n",
        "\n",
        "    if not important_features:\n",
        "        print(f\"  Warning: No features met the importance threshold for {target_label}. Using original features.\")\n",
        "        important_features = [col for col in train_df.columns if 'A' <= col <= 'N' or col == 'time']\n",
        "\n",
        "    print(f\"Selected {len(important_features)} Features for {target_label}: {important_features}\")\n",
        "\n",
        "    train_subset = train_df_featured[important_features + [target_label]]\n",
        "    test_subset = test_df_featured[important_features]\n",
        "\n",
        "    main_model_path = f\"AutogluonModels_{target_label}\"\n",
        "    print(f\"  Training main model for {target_label}. Results will be in '{main_model_path}'\")\n",
        "\n",
        "    predictor = TabularPredictor(\n",
        "        label=target_label,\n",
        "        path=main_model_path,\n",
        "        eval_metric='r2' \n",
        "    ).fit(\n",
        "        train_data=train_subset,\n",
        "        time_limit=TIME_LIMIT_PER_MODEL,\n",
        "        presets='best_quality',\n",
        "        hyperparameters={'GBM': {}, 'CAT': {}, 'XGB': {}, 'RF': {}, 'XT': {}}\n",
        "    )\n",
        "\n",
        "    print(f\"  Predicting {target_label} on test data...\")\n",
        "    all_predictions[target_label] = predictor.predict(test_subset)\n",
        "\n",
        "    print(f\"  Getting validation R² score for {target_label}...\")\n",
        "    leaderboard = predictor.leaderboard(train_subset, silent=True)\n",
        "    best_model_score = leaderboard.iloc[0]['score_val']\n",
        "    validation_scores[target_label] = best_model_score\n",
        "\n",
        "    print(f\"  Leaderboard for {target_label}:\")\n",
        "    print(leaderboard)\n",
        "\n",
        "\n",
        "print(\"\\n--- Calculating Final Validation R² Scores ---\")\n",
        "\n",
        "if 'Y1' in validation_scores and 'Y2' in validation_scores:\n",
        "    r2_y1 = validation_scores['Y1']\n",
        "    r2_y2 = validation_scores['Y2']\n",
        "\n",
        "    final_r2_score = (r2_y1 + r2_y2) / 2\n",
        "\n",
        "    print(f\"Validation R² Score for Y1 (from Leaderboard): {r2_y1:.4f}\")\n",
        "    print(f\"Validation R² Score for Y2 (from Leaderboard): {r2_y2:.4f}\")\n",
        "    print(f\"Final Average Validation R² Score: {final_r2_score:.4f}\")\n",
        "else:\n",
        "    print(\"Could not calculate final scores because one or both targets were not processed.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Creating Final Submission File ---\")\n",
        "\n",
        "final_predictions_df = pd.DataFrame(all_predictions)\n",
        "final_predictions_df['id'] = test_ids\n",
        "final_predictions_df = final_predictions_df[['id'] + TARGET_LABELS]\n",
        "\n",
        "final_predictions_df.to_csv('submission_autogluon.csv', index=False)\n",
        "\n",
        "print(\"Submission file created: submission_autogluon_FINAL.csv\")\n",
        "print(final_predictions_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIxRVkliiJSz",
        "outputId": "c6fee4ef-c772-4f4d-b531-8b159a172b8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(15996, 3)\n"
          ]
        }
      ],
      "source": [
        "sub = pd.read_csv('submission_autogluon.csv')\n",
        "print(sub.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLsDJdWioG9N",
        "outputId": "8a33d92b-f577-4c61-8e94-cb37df28d7da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    1\n",
            "1    2\n",
            "2    3\n",
            "3    4\n",
            "4    5\n",
            "Name: id, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Replace NaN values in 'id' column with a sequence of integers starting from 1\n",
        "sub['id'] = range(1, len(sub) + 1)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(sub['id'].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgDDps5jodKB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
